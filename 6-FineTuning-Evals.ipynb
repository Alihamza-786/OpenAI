{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c98eb94a-4e80-4dbc-8def-5948395b8dea",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "Fine-tune models for better results and efficiency.\n",
    "Fine-tuning lets you get more out of the models available through the API by providing:\n",
    "\n",
    "- Higher quality results than prompting\n",
    "- Ability to train on more examples than can fit in a prompt\n",
    "- Token savings due to shorter prompts\n",
    "- Lower latency requests\n",
    "\n",
    "Using demonstrations to show how to perform a task is often called \"few-shot learning.\"\n",
    "Fine-tuning improves on __few-shot learning__ by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide as many examples in the prompt. This saves costs and enables lower-latency requests.\n",
    "\n",
    "At a high level, fine-tuning involves the following steps:\n",
    "\n",
    "1. Prepare and upload training data\n",
    "2. Train a new fine-tuned model\n",
    "3. Evaluate results and go back to step 1 if needed\n",
    "4. Use your fine-tuned model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6b417-7f25-4c79-a791-ff02cbd564fe",
   "metadata": {},
   "source": [
    "### Which models can be fine-tuned?\n",
    "Fine-tuning is currently available for the following models:\n",
    "\n",
    "- gpt-4o-2024-08-06\n",
    "- gpt-4o-mini-2024-07-18\n",
    "- gpt-4-0613\n",
    "- gpt-3.5-turbo-0125\n",
    "- gpt-3.5-turbo-1106\n",
    "- gpt-3.5-turbo-0613"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ddae0-4a1c-43a8-96b3-192b4bc7451f",
   "metadata": {},
   "source": [
    "### When to use fine-tuning\n",
    "Fine-tuning OpenAI text generation models can make them better for specific applications, but it requires a careful investment of time and effort. We recommend first attempting to get good results with prompt engineering, prompt chaining (breaking complex tasks into multiple prompts), and function calling\n",
    "\n",
    "### Common use cases\n",
    "Some common use cases where fine-tuning can improve results:\n",
    "\n",
    "- Setting the style, tone, format, or other qualitative aspects\n",
    "- Improving reliability at producing a desired output\n",
    "- Correcting failures to follow complex prompts\n",
    "- Handling many edge cases in specific ways\n",
    "- Performing a new skill or task that’s hard to articulate in a prompt\n",
    "  \n",
    "One high-level way to think about these cases is when it’s easier to \"show, not tell\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9462f-d5e9-4985-910f-7d16ed92cb16",
   "metadata": {},
   "source": [
    "### Preparing your dataset\n",
    "Each example in the dataset should be a conversation in the same format as our Chat Completions API, specifically a list of messages where each message has a role, content, and optional name.\n",
    "### Example format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5fceb-3412-40ec-8b96-8bbdfabf77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f36db-c976-47ee-83cc-09ccb3adee15",
   "metadata": {},
   "source": [
    "### Multi-turn chat examples\n",
    "Examples in the chat format can have multiple messages with the assistant role. The default behavior during fine-tuning is to train on all assistant messages within a single example. To skip fine-tuning on specific assistant messages, a weight key can be added disable fine-tuning on that message, allowing you to control which assistant messages are learned. The allowed values for weight are currently 0 or 1. Some examples using weight for the chat format are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ff78e3-c2aa-4381-a803-2dbb08251ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\", \"weight\": 1}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"William Shakespeare\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\", \"weight\": 1}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"384,400 kilometers\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\", \"weight\": 1}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0740ab-594c-4f23-ab2a-5fdbf1cadbed",
   "metadata": {},
   "source": [
    "### Example count recommendations\n",
    "To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples with gpt-4o-mini and gpt-3.5-turbo, but the right number varies greatly based on the exact use case.\n",
    "We recommend starting with __50__ well-crafted demonstrations and seeing if the model shows signs of improvement after fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851dd9a5-f29c-4f1e-9698-7ae39f3393ee",
   "metadata": {},
   "source": [
    "### Train and test splits\n",
    "After collecting the initial dataset, we recommend splitting it into a training and test portion. When submitting a fine-tuning job with both training and test files, we will provide statistics on both during the course of training. These statistics will be your initial signal of how much the model is improving. Additionally, constructing a test set early on will be useful in making sure you are able to evaluate the model after training, by generating samples on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4be42-c5b8-4395-bc69-42a7c114c3a7",
   "metadata": {},
   "source": [
    "### Token limits\n",
    "Token limits depend on the model you select. Here is an overview of the maximum inference context length and training examples context length for gpt-4o-mini and gpt-3.5-turbo models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b61c7f3-98a9-4805-ad15-b4073801b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Model\t                Inference context length\tTraining examples context length\n",
    "gpt-4o-2024-08-06\t    128,000 tokens\t            65,536 tokens (128k coming soon)\n",
    "gpt-4o-mini-2024-07-18\t128,000 tokens\t            65,536 tokens (128k coming soon)\n",
    "gpt-3.5-turbo-0125\t    16,385 tokens\t            16,385 tokens\n",
    "gpt-3.5-turbo-1106\t    16,385 tokens\t            16,385 tokens\n",
    "gpt-3.5-turbo-0613\t    16,385 tokens\t            4,096 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a55f0-3c7d-4440-94d4-96a669e44cf7",
   "metadata": {},
   "source": [
    "Examples longer than the default will be truncated to the maximum context length which removes tokens from the end of the training example(s). To be sure that your entire training example fits in context, consider checking that the total token counts in the message contents are under the limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca57c9f-9bbd-4ca0-a41e-31745d228573",
   "metadata": {},
   "source": [
    "### Check data formatting\n",
    "Once you have compiled a dataset and before you create a fine-tuning job, it is important to check the data formatting. To do this, we created a simple Python script which you can use to find potential errors, review token counts, and estimate the cost of a fine-tuning job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67bba33-704f-4217-8e22-751b5cf42e8c",
   "metadata": {},
   "source": [
    "### [Fine-tuning data format validation](https://cookbook.openai.com/examples/chat_finetuning_data_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67412453-6f95-46db-8969-8e7c8dcafada",
   "metadata": {},
   "source": [
    "### Upload a training file\n",
    "Once you have the data validated, the file needs to be uploaded using the Files API in order to be used with a fine-tuning jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d9236-4803-4d6b-8e83-b77aee8e645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning job with DPO\n",
    "#DPO stands for Direct Preference Optimization. It is a method used to fine-tune AI models based on human preferences without needing a reward model.\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-all-about-the-weather\", #The training_file parameter is the file ID that was returned when the training file was uploaded to the OpenAI API\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    method={\n",
    "        \"type\": \"dpo\",\n",
    "        \"dpo\": {\n",
    "            \"hyperparameters\": {\"beta\": 0.1},\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23db9dd-248d-427a-b04f-ff17756864bd",
   "metadata": {},
   "source": [
    "### DPO (Direct Preference Optimization)\n",
    "DPO is a fine-tuning method that trains a model using human preference data without needing a separate reward model (unlike RLHF).\n",
    "\n",
    "#### How it works?\n",
    "\n",
    "- Humans rank two responses (which one is better).\n",
    "- The model learns to prefer better responses over worse ones.\n",
    "- It directly adjusts the model using these preferences without reinforcement learning.\n",
    "#### Why use DPO?\n",
    "\n",
    "- Faster than RLHF (no reward model needed).\n",
    "- More stable (avoids complex RL training).\n",
    "\n",
    "### SFT (Supervised Fine-Tuning)\n",
    "SFT is basic fine-tuning where the model is trained on labeled data (input → correct output).\n",
    "\n",
    "#### How it works?\n",
    "\n",
    "- You give the model pairs of inputs and correct answers.\n",
    "- It learns to mimic the labeled data.\n",
    "- This is the first step before applying RLHF or DPO.\n",
    "#### Why use SFT?\n",
    "\n",
    "- It teaches the model good responses before preference tuning.\n",
    "- Used for instruction tuning (e.g., making ChatGPT follow prompts better)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f955d9-c7d9-42ac-805e-75f6e7c7936d",
   "metadata": {},
   "source": [
    "In DPO (Direct Preference Optimization), __beta__ is a hyperparameter that controls the trade-off between:\n",
    "\n",
    "1. Staying close to the original model (before fine-tuning).\n",
    "2. Adapting to human preferences (based on preference data).\n",
    "#### What Does beta: 0.1 Mean?\n",
    "The beta hyperparameter is a new option that is only available for DPO. It's a floating point number between 0 and 2 that controls how strictly the new model will adhere to its previous behavior, versus aligning with the provided preferences.\n",
    "- A high number will be more conservative (favoring previous behavior)\n",
    "- A lower number will be more aggressive (favor the newly provided preferences more often)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72f3f1-d212-4072-bd79-41d56c92606d",
   "metadata": {},
   "source": [
    "### RLHF (Reinforcement Learning from Human Feedback)\n",
    "RLHF is a fine-tuning method where a model is trained using human feedback and reinforcement learning.\n",
    "\n",
    "#### How RLHF Works?\n",
    "- Supervised Fine-Tuning (SFT) → Train the model on labeled data (input → correct output).\n",
    "- Preference Data Collection → Humans rank multiple responses from the model (which is better).\n",
    "- Train a Reward Model (RM) → A new model learns to predict human preferences from ranked responses.\n",
    "- Reinforcement Learning (RL) → The main model is fine-tuned using PPO (Proximal Policy Optimization) to maximize the reward model's score.\n",
    "#### Why RLHF?\n",
    "- Helps the model align with human values and avoid harmful responses.\n",
    "- Used in ChatGPT, GPT-4, and other AI assistants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fbddf3-442d-4af4-af2e-b7fe91a0130a",
   "metadata": {},
   "source": [
    "### Size limits and large uploads\n",
    "The maximum upload size is 512 MB using the Files API. You can upload files up to 8 GB in multiple parts using the Uploads API. We recommend starting small, as you don't need a lot of data to see improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c93d0-eaa1-495c-9960-a47d66bcd99f",
   "metadata": {},
   "source": [
    "### Create a fine-tuned model\n",
    "After ensuring you have the right amount and structure for your dataset, and have uploaded the file, the next step is to create a fine-tuning job. We support creating fine-tuning jobs via the fine-tuning UI or programmatically.\n",
    "\n",
    "To start a fine-tuning job using the OpenAI SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6daa336-b1e7-4d48-88b8-3aa0d59b639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create fine-tuning job\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "client.file_tuning.jobs.create(\n",
    "    training_file = \"file-abc123\", #The training_file parameter is the file ID that was returned when the training file was uploaded to the OpenAI API\n",
    "    model = \"gpt-4o-mini-2024-07-08\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038cc13-8123-4718-911d-06688a10fbc9",
   "metadata": {},
   "source": [
    "In addition to creating a fine-tuning job, you can also list existing jobs, retrieve the status of a job, or cancel a job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a5ccac-59d8-40c2-82a2-68c72083f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with fine-tuning jobs\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(\"ftjob-abc123\")\n",
    "\n",
    "# Cancel a job\n",
    "client.fine_tuning.jobs.cancel(\"ftjob-abc123\")\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-abc123\", limit=10)\n",
    "\n",
    "# Delete a fine-tuned model\n",
    "client.models.delete(\"ft:gpt-3.5-turbo:acemeco:suffix:abc123\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34087474-8546-4740-981f-5f69c7e11a5f",
   "metadata": {},
   "source": [
    "### Use a fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ecb38-4794-489b-b085-681bf3ffe490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use fine-tuned model\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"ft:gpt-4o-mini:my-org:custom_suffix:id\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d860be-b580-4725-8955-537bac7ca4dd",
   "metadata": {},
   "source": [
    "### Use a checkpointed model\n",
    "In addition to creating a final fine-tuned model at the end of each fine-tuning job, OpenAI will create one full model checkpoint for you at the end of each training epoch. These checkpoints are themselves full models that can be used within our completions and chat-completions endpoints. Checkpoints are useful as they potentially provide a version of your fine-tuned model from before it experienced overfitting.\n",
    "\n",
    "To access these checkpoints,\n",
    "\n",
    "1. Wait until a job succeeds, which you can verify by querying the status of a job.\n",
    "2. Query the checkpoints endpoint with your fine-tuning job ID to access a list of model checkpoints for the fine-tuning job.\n",
    "   \n",
    "For each checkpoint object, you will see the fine_tuned_model_checkpoint field populated with the name of the model checkpoint. You may now use this model just like you would with the final fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4d27e-2d77-425c-a662-cd40a8c5efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"object\": \"fine_tuning.job.checkpoint\",\n",
    "    \"id\": \"ftckpt_zc4Q7MP6XxulcVzj4MZdwsAB\",\n",
    "    \"created_at\": 1519129973,\n",
    "    \"fine_tuned_model_checkpoint\": \"ft:gpt-3.5-turbo-0125:my-org:custom-suffix:96olL566:ckpt-step-2000\",\n",
    "    \"metrics\": {\n",
    "        \"full_valid_loss\": 0.134,\n",
    "        \"full_valid_mean_token_accuracy\": 0.874\n",
    "    },\n",
    "    \"fine_tuning_job_id\": \"ftjob-abc123\",\n",
    "    \"step_number\": 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e9d8a-3512-4b38-b8e1-77c115562c3c",
   "metadata": {},
   "source": [
    "Each checkpoint will specify its:\n",
    "\n",
    "- step_number: The step at which the checkpoint was created (where each epoch is number of steps in the training set divided by the batch size)\n",
    "- metrics: an object containing the metrics for your fine-tuning job at the step when the checkpoint was created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605c131-6087-4f77-9b33-8a9439c11138",
   "metadata": {},
   "source": [
    "### Analyzing your fine-tuned model\n",
    "We provide the following training metrics computed over the course of training:\n",
    "\n",
    "- training loss\n",
    "- training token accuracy\n",
    "- valid loss\n",
    "- valid token accuracy\n",
    "  \n",
    "Valid loss and valid token accuracy are computed in two different ways - on a small batch of the data during each step, and on the full valid split at the end of each epoch. The full valid loss and full valid token accuracy metrics are the most accurate metric tracking the overall performance of your model. These statistics are meant to provide a sanity check that training went smoothly (loss should decrease, token accuracy should increase). While an active fine-tuning jobs is running, you can view an event object which contains some useful metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5861d-c968-4f0a-8bad-4455124e6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"object\": \"fine_tuning.job.event\",\n",
    "    \"id\": \"ftevent-abc-123\",\n",
    "    \"created_at\": 1693582679,\n",
    "    \"level\": \"info\",\n",
    "    \"message\": \"Step 300/300: training loss=0.15, validation loss=0.27, full validation loss=0.40\",\n",
    "    \"data\": {\n",
    "        \"step\": 300,\n",
    "        \"train_loss\": 0.14991648495197296,\n",
    "        \"valid_loss\": 0.26569826706596045,\n",
    "        \"total_steps\": 300,\n",
    "        \"full_valid_loss\": 0.4032616495084362,\n",
    "        \"train_mean_token_accuracy\": 0.9444444179534912,\n",
    "        \"valid_mean_token_accuracy\": 0.9565217391304348,\n",
    "        \"full_valid_mean_token_accuracy\": 0.9089635854341737\n",
    "    },\n",
    "    \"type\": \"metrics\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bca911-cdc9-43cf-b962-e922e812e1ce",
   "metadata": {},
   "source": [
    "After a fine-tuning job has finished, you can also see metrics around how the training process went by querying a fine-tuning job, extracting a file ID from the result_files, and then retrieving that files content. Each results CSV file has the following columns: step, train_loss, train_accuracy, valid_loss, and valid_mean_token_accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f1a017-3140-46af-95db-f1b3293388e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "step,train_loss,train_accuracy,valid_loss,valid_mean_token_accuracy\n",
    "1,1.52347,0.0,,\n",
    "2,0.57719,0.0,,\n",
    "3,3.63525,0.0,,\n",
    "4,1.72257,0.0,,\n",
    "5,1.52379,0.0,,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aadef5b-2938-4564-b85a-1b9af21ef3ec",
   "metadata": {},
   "source": [
    "You can set the hyperparameters as is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9802104-71fa-4dec-b3e1-f7a17a3a2865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting hyperparameters\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-abc123\",\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    method={\n",
    "        \"type\": \"supervised\",\n",
    "        \"supervised\": {\n",
    "            \"hyperparameters\": {\"n_epochs\": 2},\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9f5e0-432a-4f0e-ae4a-edad6c09ae0d",
   "metadata": {},
   "source": [
    "### Vision fine-tuning\n",
    "Fine-tuning is also possible with images in your JSONL files. Just as you can send one or many image inputs to chat completions, you can include those same message types within your training data. Images can be provided either as HTTP URLs or data URLs containing base64 encoded images.\n",
    "\n",
    "Here's an example of an image message on a line of your JSONL file. Below, the JSON object is expanded for readibility, but typically this JSON would appear on a single line in your data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884dcbf2-7e7c-4a17-9279-e00fc5680e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are an assistant that identifies uncommon cheeses.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is this cheese?\" },\n",
    "    { \"role\": \"user\", \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/3/36/Danbo_Cheese.jpg\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    { \"role\": \"assistant\", \"content\": \"Danbo\" }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a65767-f74a-435e-bee4-0c936bcf4e55",
   "metadata": {},
   "source": [
    "### Image dataset requirements\n",
    "#### Size\n",
    "- Your training file can contain a maximum of 50,000 examples that contain images (not including text examples).\n",
    "- Each example can have at most 10 images.\n",
    "- Each image can be at most 10 MB.\n",
    "#### Format\n",
    "- Images must be JPEG, PNG, or WEBP format.\n",
    "- Your images must be in the RGB or RGBA image mode.\n",
    "- You cannot include images as output from messages with the assistant role.\n",
    "#### Content moderation policy\n",
    "We scan your images before training to ensure that they comply with our usage policy. This may introduce latency in file validation before fine-tuning begins.\n",
    "\n",
    "Images containing the following will be excluded from your dataset and not used for training:\n",
    "\n",
    "- People\n",
    "- Faces\n",
    "- Children\n",
    "- CAPTCHAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751450a-1b8e-410e-9191-acd673203f0e",
   "metadata": {},
   "source": [
    "### Reducing training cost\n",
    "If you set the __detail__ parameter for an image to __low__, the image is resized to 512 by 512 pixels and is only represented by 85 tokens regardless of its size. This will reduce the cost of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5256125-ce96-4231-b0ea-2a2bed0f926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"type\": \"image_url\",\n",
    "    \"image_url\": {\n",
    "        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/3/36/Danbo_Cheese.jpg\",\n",
    "        \"detail\": \"low\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173a2f3-8c95-4da4-8d97-ddb67ca42eb4",
   "metadata": {},
   "source": [
    "### Preference fine-tuning\n",
    "Direct Preference Optimization (DPO) fine-tuning allows you to fine-tune models based on prompts and pairs of responses. This approach enables the model to learn from human preferences, optimizing for outputs that are more likely to be favored. Note that we currently support text-only DPO fine-tuning.\n",
    "\n",
    "#### Preparing your dataset for DPO\n",
    "\n",
    "Each example in your dataset should contain:\n",
    "\n",
    "- A prompt, like a user message.\n",
    "- A preferred output (an ideal assistant response).\n",
    "- A non-preferred output (a suboptimal assistant response).\n",
    "\n",
    "The data should be formatted in JSONL format, with each line representing an example in the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08804c6-635d-4e9e-9b18-97d01deecbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"input\": {\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, can you tell me how cold San Francisco is today?\"\n",
    "      }\n",
    "    ],\n",
    "    \"tools\": [],\n",
    "    \"parallel_tool_calls\": true\n",
    "  },\n",
    "  \"preferred_output\": [\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Today in San Francisco, it is not quite cold as expected. Morning clouds will give away to sunshine, with a high near 68°F (20°C) and a low around 57°F (14°C).\"\n",
    "    }\n",
    "  ],\n",
    "  \"non_preferred_output\": [\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"It is not particularly cold in San Francisco today.\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0237e5d1-b779-474d-a5be-eb11255203aa",
   "metadata": {},
   "source": [
    "#### Stacking methods: Supervised + DPO\n",
    "Currently, OpenAI offers Supervised Fine-Tuning (SFT) as the default method for fine-tuning jobs. Performing SFT on your preferred responses (or a subset) before running another DPO job afterwards can significantly enhance model alignment and performance. By first fine-tuning the model on the desired responses, it can better identify correct patterns, providing a strong foundation for DPO to refine behavior.\n",
    "\n",
    "A recommended workflow is as follows:\n",
    "\n",
    "- Fine-tune the base model with SFT using a subset of your preferred responses. Focus on ensuring the data quality and representativeness of the tasks.\n",
    "- Use the SFT fine-tuned model as the starting point, and apply DPO to adjust the model based on preference comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703e8bd-5d61-4cbd-b2d0-5055f115f23c",
   "metadata": {},
   "source": [
    "The __beta__ hyperparameter is a new option that is only available for DPO. It's a floating point number between __0__ and __2__ that controls how strictly the new model will adhere to its previous behavior, versus aligning with the provided preferences. A high number will be more conservative (favoring previous behavior), and a lower number will be more aggressive (favor the newly provided preferences more often)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f699f24-40e5-4818-a4e1-c9810796f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning job with DPO\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-all-about-the-weather\",\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    method={\n",
    "        \"type\": \"dpo\",\n",
    "        \"dpo\": {\n",
    "            \"hyperparameters\": {\"beta\": 0.1},\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220afbc1-1a7f-4df7-9636-71f4be33f11e",
   "metadata": {},
   "source": [
    "### Fine-tuning examples\n",
    "Now that we have explored the basics of the fine-tuning API, let’s look at going through the fine-tuning lifecycle for a few different use cases.\n",
    "#### Style and tone\n",
    "In this example, we will explore how to build a fine-tuned model which gets the model follow specific style and tone guidance beyond what is possible with prompting alone.\n",
    "\n",
    "To begin, we create a sample set of messages showing what the model should which in this case is misspelled words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322d7a0-b98b-4ad6-91da-a03022824481",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b53b9-7267-4150-8f31-9424d15118f1",
   "metadata": {},
   "source": [
    "If you want to follow along and create a fine-tuned model yourself, you will need at least 10 examples.\n",
    "\n",
    "After getting the data that will potentially improve the model, the next step is to check if the data meets all the formatting requirements.\n",
    "\n",
    "Now that we have the data formatted and validated, the final training step is to kick off a job to create the fine-tuned model. You can do this via the OpenAI CLI or one of our SDKs as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081026c9-5c5a-48bf-8d0f-8b31f2e61a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "file = client.files.create(\n",
    "  file=open(\"marv.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=file.id,\n",
    "  model=\"gpt-4o-mini-2024-07-18\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3d70d-27bb-4531-b828-7dfcb658cd3a",
   "metadata": {},
   "source": [
    "Once the training job is done, you will be able to use your fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884512d1-fb27-414d-98c1-776328aee71c",
   "metadata": {},
   "source": [
    "#### Structured output\n",
    "Another type of use case which works really well with fine-tuning is getting the model to provide structured information, in this case about sports headlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0fd83-1933-4034-9887-6f61a7096294",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Given a sports headline, provide the following fields in a JSON dict, where applicable: \\\"player\\\" (full name), \\\"team\\\", \\\"sport\\\", and \\\"gender\\\".\"}, {\"role\": \"user\", \"content\": \"Sources: Colts grant RB Taylor OK to seek trade\"}, {\"role\": \"assistant\", \"content\": \"{\\\"player\\\": \\\"Jonathan Taylor\\\", \\\"team\\\": \\\"Colts\\\", \\\"sport\\\": \\\"football\\\", \\\"gender\\\": \\\"male\\\" }\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Given a sports headline, provide the following fields in a JSON dict, where applicable: \\\"player\\\" (full name), \\\"team\\\", \\\"sport\\\", and \\\"gender\\\".\"}, {\"role\": \"user\", \"content\": \"OSU 'split down middle' on starting QB battle\"}, {\"role\": \"assistant\", \"content\": \"{\\\"player\\\": null, \\\"team\\\": \\\"OSU\\\", \\\"sport\\\": \\\"football\\\", \\\"gender\\\": null }\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b4710b-1e65-4ff7-9063-a1f991baf182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "file = client.files.create(\n",
    "  file=open(\"sports-context.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=file.id,\n",
    "  model=\"gpt-4o-mini-2024-07-18\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8ce9c-f368-4bb3-a56e-5ffd1af23222",
   "metadata": {},
   "source": [
    "Once the training job is done, you will be able to use your fine-tuned model and make a request that looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a1055-8578-414f-a27d-be06409da1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model = \"ft:gpt-4o-mini:my-org:custom_suffix:id\",\n",
    "    messages= [\n",
    "         {\"role\": \"system\", \"content\": \"Given a sports headline, provide the following fields in a JSON dict, where applicable: player (full name), team, sport, and gender\"},\n",
    "        {\"role\": \"user\", \"content\": \"Richardson wins 100m at worlds to cap comeback\"}\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d87600-ca2f-431b-b18b-5a0ff87b2235",
   "metadata": {},
   "source": [
    "Based on the formatted training data, the response should look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4557a3-9d2e-4b4f-8737-0dca2738a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"player\": \"Sha'Carri Richardson\",\n",
    "    \"team\": null,\n",
    "    \"sport\": \"track and field\",\n",
    "    \"gender\": \"female\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d7672-f0a6-46d6-9d94-e2dff3e7ae85",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "Fine-tuning a model with tool calling examples can allow you to:\n",
    "\n",
    "- Get similarly formatted responses even when the full tool definition isn't present\n",
    "- Get more accurate and consistent outputs\n",
    "Format your examples as shown, with each line including a list of \"messages\" and an optional list of \"tools\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4114ff-8022-45b9-97bf-fb2187939f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is weather in San Francisco?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": \"call_id\",\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"get_current_weather\",\n",
    "                        \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celsius\\\"}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and country e.g. San Francisco and USA.\"\n",
    "                        },\n",
    "                        \"format\": {\"type\": \"string\", \"enum\":[\"celcius\", \"fahrenheit\"]}\n",
    "                    },\n",
    "                    \"required\": [\"location\", \"format\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a347dab-612a-4877-ada3-16af0374613a",
   "metadata": {},
   "source": [
    "If your goal is to use less tokens, some useful techniques are:\n",
    "\n",
    "- Omit function and parameter descriptions: remove the description field from function and parameters\n",
    "- Omit parameters: remove the entire properties field from the parameters object\n",
    "- Omit function entirely: remove the entire function object from the functions array\n",
    "  \n",
    "If your goal is to maximize the correctness of the function calling output, we recommend using the same tool definitions for both training and querying the fine-tuned model.\n",
    "\n",
    "Fine-tuning on function calling can also be used to customize the model's response to function outputs. To do this you can include a function response message and an assistant message interpreting that response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897f612-f342-4df6-8889-4554b269c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather in San Francisco?\"},\n",
    "        {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_id\", \"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celsius\\\"}\"}}]}\n",
    "        {\"role\": \"tool\", \"tool_call_id\": \"call_id\", \"content\": \"21.0\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"It is 21 degrees celsius in San Francisco, CA\"}\n",
    "    ],\n",
    "    \"tools\": [] // same as before\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af721c0-21dd-4903-93d4-239cb9b2d60e",
   "metadata": {},
   "source": [
    "#### Function calling\n",
    "function_call and functions have been deprecated in favor of tools it is recommended to use the tools parameter instead.\n",
    "\n",
    "The chat completions API supports function calling. Including a long list of functions in the completions API can consume a considerable number of prompt tokens and sometimes the model hallucinates or does not provide valid JSON output.\n",
    "\n",
    "Fine-tuning a model with function calling examples can allow you to:\n",
    "\n",
    "- Get similarly formatted responses even when the full function definition isn't present\n",
    "- Get more accurate and consistent outputs\n",
    "\n",
    "Format your examples as shown, with each line including a list of \"messages\" and an optional list of \"functions\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675173c8-79da-4217-8e07-0abb4bff1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"messages\": [\n",
    "        { \"role\": \"user\", \"content\": \"What is the weather in San Francisco?\" },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"function_call\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celsius\\\"}\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and country, eg. San Francisco, USA\"\n",
    "                    },\n",
    "                    \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"] }\n",
    "                },\n",
    "                \"required\": [\"location\", \"format\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d4b45-aeaf-45e3-8267-5f667bd5c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather in San Francisco?\"},\n",
    "        {\"role\": \"assistant\", \"function_call\": {\"name\": \"get_current_weather\", \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celsius\\\"}\"}}\n",
    "        {\"role\": \"function\", \"name\": \"get_current_weather\", \"content\": \"21.0\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"It is 21 degrees celsius in San Francisco, CA\"}\n",
    "    ],\n",
    "    \"functions\": [] // same as before\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121c7bf-d3b2-4103-a963-1ce8d8cefc82",
   "metadata": {},
   "source": [
    "### Weights and Biases Integration\n",
    "Weights and Biases (W&B) is a popular tool for tracking machine learning experiments. You can use the OpenAI integration with W&B to track your fine-tuning jobs in W&B. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f6d68-05f4-446e-a2e0-a0b7150e2ab9",
   "metadata": {},
   "source": [
    "### When should I use fine-tuning vs embeddings / retrieval augmented generation?\n",
    "Embeddings with retrieval is best suited for cases when you need to have a large database of documents with relevant context and information.\n",
    "\n",
    "By default OpenAI’s models are trained to be helpful generalist assistants. Fine-tuning can be used to make a model which is narrowly focused, and exhibits specific ingrained behavior patterns. Retrieval strategies can be used to make new information available to a model by providing it with relevant context before generating its response. Retrieval strategies are not an alternative to fine-tuning and can in fact be complementary to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b13ef3-2573-464d-9cc5-ec42d34a396c",
   "metadata": {},
   "source": [
    "## Model distillation\n",
    "Improve smaller models with distillation techniques.\n",
    "Model Distillation allows you to leverage the outputs of a large model to fine-tune a smaller model, enabling it to achieve similar performance on a specific task. This process can significantly reduce both cost and latency, as smaller models are typically more efficient.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "- Store high-quality outputs of a large model using the __store__ parameter in the chat completions API to store them.\n",
    "- Evaluate the stored completions with both the large and the small model to establish a baseline.\n",
    "- Select the stored completions that you'd like to use to for distillation and use them to fine-tune the smaller model.\n",
    "- Evaluate the performance of the fine-tuned model to see how it compares to the large model.\n",
    "Let's go through these steps to see how it's done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1fbe7-2520-4214-bacd-569ac07d9c05",
   "metadata": {},
   "source": [
    "### Store high-quality outputs of a large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d54851-31dd-497a-b0d9-4b7311df1a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#javascript\n",
    "import OpenAI from \"openai\";\n",
    "const openai = new OpenAI();\n",
    "\n",
    "const response = await openai.chat.completions.create({\n",
    "  model: \"gpt-4o\",\n",
    "  messages: [\n",
    "    { role: \"system\", content: \"You are a corporate IT support expert.\" },\n",
    "    { role: \"user\", content: \"How can I hide the dock on my Mac?\"},\n",
    "  ],\n",
    "  store: true,\n",
    "  metadata: {\n",
    "    role: \"manager\",\n",
    "    department: \"accounting\",\n",
    "    source: \"homepage\"\n",
    "  }\n",
    "});\n",
    "\n",
    "console.log(response.choices[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567e57b-6212-4864-92d4-0049547801ce",
   "metadata": {},
   "source": [
    "### Evaluate to establish a baseline\n",
    "You can use your stored completions to evaluate the performance of both the larger model and a smaller model on your task to establish a baseline. This can be done using the evals product.\n",
    "\n",
    "### Create training dataset to fine-tune smaller model\n",
    "Next you can select a subset of your stored completions to use as training data for fine-tuning a smaller model like gpt-4o-mini. Filter your stored completions to those that you would like to use to train the small model, and click the \"Distill\" button.\n",
    "\n",
    "### Evaluate the fine-tuned small model\n",
    "When your fine-tuning job is complete, you can run evals against it to see how it stacks up against the base small and large models. You can select fine-tuned models in the Evals product to generate new completions with the fine-tuned small model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeec000-9e84-4463-97d1-4bee11761588",
   "metadata": {},
   "source": [
    "## Evaluating model performance\n",
    "\n",
    "Test and improve model outputs through evaluations.\n",
    "Evaluations (often called evals) help you develop high-quality LLM applications by ensuring that the results you generate with a model meet accuracy criteria that you specify. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7cf67-79cb-4acd-834b-801c58547e0a",
   "metadata": {},
   "source": [
    "### Understand your objective\n",
    "Before you begin, you should understand the behavior you want from the model. Here are some questions you should be able to answer:\n",
    "\n",
    "- What kind of output do I want the model to generate?\n",
    "- What kinds of inputs should the model be able to handle?\n",
    "- How could I tell whether or not my model output is accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a787cd5e-257d-4833-abd2-659bffac1fcb",
   "metadata": {},
   "source": [
    "### Understand your objective\n",
    "Before you begin, you should understand the behavior you want from the model. Here are some questions you should be able to answer:\n",
    "\n",
    "- What kind of output do I want the model to generate?\n",
    "- What kinds of inputs should the model be able to handle?\n",
    "- How could I tell whether or not my model output is accurate?\n",
    "  \n",
    "The answers to these questions will dictate how you build your eval, and what kind of data you need to assemble before you can evaluate model performance. Let's make this more concrete with a real example.\n",
    "\n",
    "#### Example objective - sentiment analysis of movie reviews\n",
    "Let's say you are building an LLM application to perform sentiment analysis on movie reviews from IMDB. You'd like to give the model the text of a movie review, and have it output 1 if it's a positive review, and 0 if it's a negative review. For this use case, let's answer the three questions posed above:\n",
    "\n",
    "What kind of output do I want the model to generate?\n",
    "\n",
    "In this case, you want the model to generate just a 1 for positive reviews, and 0 for negative reviews. You don't want the model to generate anything else besides these two numbers.\n",
    "\n",
    "What kinds of inputs should the model be able to handle?\n",
    "\n",
    "The model should be able to process user-generated movie review text. Based on the text of the reviews, the model should be able to ascertain whether or not the overall tone of the review is positive or negative. For testing purposes, we will assume that all the reviews being tested are in English.\n",
    "\n",
    "How could I tell whether or not my model output is accurate?\n",
    "\n",
    "There are a few potential ways we could tell if the model output is accurate:\n",
    "\n",
    "- We could ask another (ideally larger or more powerful) model to analyze the same review. If the other model agreed with our model's output, that could be a strong indicator of success.\n",
    "- We could compare the model output to human-created output on the same review, which we could trust to be accurate. Human-labeled data could be considered ground truth, which is the reality we want to test our model outputs against.\n",
    "  \n",
    "The second option is likely to be most accurate - so if we have the ability to test against human-labeled data as our ground truth, that's probably the best option. Model-graded outputs can be useful when human-labeled data is not available, though.\n",
    "\n",
    "Now that we have a firm grasp on what we want the model to generate, what kinds of inputs we want to test with, and how we'd know if the ouputs are accurate, we can move on to collecting the data we need for our eval.\n",
    "\n",
    "#### Assemble test data\n",
    "Getting good test data inputs is arguably the hardest part of building evals. That's because good test data must be representative of the kinds of inputs the model will receive in production.\n",
    "\n",
    "Consider our movie review use case. If all our test cases were negative reviews written by the same person, it would not be a very good test dataset. We'd ideally want the percentage of positive and negative reviews to match the proportions we observe in reality, and to be written by a diverse range of authors.\n",
    "\n",
    "When generating a test dataset, you have a few options:\n",
    "\n",
    "- You can find an existing open source dataset that matches your use case\n",
    "- You can create a synthetic dataset, either by having a human or a model generate test inputs that meet your specifications\n",
    "- You can create a dataset from real production usage of your application\n",
    "  \n",
    "The third option is ideal, since it is likely to resemble the reality your application will face. If you are already using OpenAI, you might consider using Stored Completions for your API traffic. Setting store: true on your completions will make them show up here in the dashboard, where you can filter them to create a dataset for evals, fine-tuning, or distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4bc2f-d597-47c6-bfa4-96f27a720d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import OpenAI from \"openai\";\n",
    "const openai = new OpenAI();\n",
    "\n",
    "const response = await openai.chat.completions.create({\n",
    "  model: \"gpt-4o\",\n",
    "  messages: [\n",
    "    { role: \"system\", content: \"You are a corporate IT support expert.\" },\n",
    "    { role: \"user\", content: \"How can I hide the dock on my Mac?\"},\n",
    "  ],\n",
    "  store: true,\n",
    "  metadata: {\n",
    "    role: \"manager\",\n",
    "    department: \"accounting\",\n",
    "    source: \"homepage\"\n",
    "  }\n",
    "});\n",
    "\n",
    "console.log(response.choices[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f96700-d3b7-47c5-b4f1-b88ad28c0d3b",
   "metadata": {},
   "source": [
    "For the purposes of this guide, however, we'll be using an open source dataset from Hugging Face. You can download a CSV version of the data [from here.](https://huggingface.co/datasets/stanfordnlp/imdb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
