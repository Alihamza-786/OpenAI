{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30d6226-2285-421f-96bd-42a470c5653a",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "OpenAI provides simple APIs to use a large language model to generate text from a prompt, as you might using ChatGPT. These models have been trained on vast quantities of data to understand multimedia inputs and natural language instructions. From these prompts, models can generate almost any kind of text response, like code, mathematical equations, structured JSON data, or human-like prose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420f171-73ae-4292-b6eb-d4e67bbcdbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku about recursion in programming.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3506b45-feba-4f0a-9345-fe0c61a12170",
   "metadata": {},
   "source": [
    "A function calls self,  \n",
    "layers deep like falling leaves,  \n",
    "endless yet finite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca00d63-3c1c-4a57-9bbb-56dd573b0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(\n",
    "    model = \"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": \"You are a helpfull assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about recursion in programming.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce29ccf-9a1b-4752-9671-d065383db8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate JSON data based on a JSON Schema\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4o-2024-08-06\",\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You extract email addresses in JSON data.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Feeling stuck? Send a message to help@mycompany.com.\"\n",
    "        }\n",
    "    ],\n",
    "    response_format = {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\":{\n",
    "            \"name\": \"email_schema\",\n",
    "            \"schema\":{\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"email\": {\n",
    "                        \"description\": \"The emiail address that appears in the input\",\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(response.choice[0].message.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d9a6d-971d-40a5-94ee-0359f07abf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a JSON response from OpenAI's API\n",
    "{\n",
    "  \"id\": \"chatcmpl-Af6LFgbOPpqu2fhGsVktc9xFaYUVh\", #A unique identifier for this chat completion request.\n",
    "  \"object\": \"chat.completion\", #Specifies that this is a chat completion.\n",
    "  \"created\": 1734359189, #A timestamp representing when the response was generated.\n",
    "\n",
    "  \"model\": \"gpt-4o-2024-08-06\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Code within a loop,  \\nFunction calls itself again,  \\nInfinite echoes.\",\n",
    "        \"refusal\": null #The AI did not refuse to generate content.\n",
    "      },\n",
    "      \"logprobs\": null, #means that log probabilities (logprobs) were not included in the response. Logprobs show how confident the AI is in each word it generates.\n",
    "      \"finish_reason\": \"stop\" #The response ended naturally (not cut off).\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d75a3-626e-440a-9fef-51964ba2af9b",
   "metadata": {},
   "source": [
    "A loop within loops,  \n",
    "calls itself infinitely,  \n",
    "stack overflow comes.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8092a8-ee40-417a-8fa2-f2fefcefc723",
   "metadata": {},
   "source": [
    "## Vision\n",
    "Several OpenAI models have vision capabilities, meaning the models can take images as input and answer questions about them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20d46d-8574-4bb5-9464-04edbecadeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e97582-cfc0-499d-87ec-f57157ab259a",
   "metadata": {},
   "source": [
    "### Uploading Base64 encoded images\n",
    "If you have an image or set of images locally, pass them to the model in Base64 encoded format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054242b4-9895-4a97-8ab4-837cd4327de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path): \n",
    "    with open(image_path, \"rb\") as image_file: # \"rb\" means read the file in binary mode (raw data, not text).read\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\") #Base64 → Converts binary data (like images) into a text format.\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"path_to_your_image.jpg\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in this image?\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa27d8d-6c9e-4864-a494-2c71f2484c80",
   "metadata": {},
   "source": [
    "### Multiple image inputs\n",
    "The Chat Completions API is capable of taking in and processing multiple image inputs, in Base64 encoded format or as an image URL. \n",
    "The model processes each image and uses information from all images to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c407c6-f7e8-41fb-bdf7-9f952890edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\":[\n",
    "                {\n",
    "                    \"type\" : \"text\",\n",
    "                    \"text\" : \"What are in this images? Is there any difference between them?\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\" : \"image_url\",\n",
    "                    \"image_url\":{\n",
    "                        \"url\": \"https://upload.wikimedia.org/wikipedia\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ffae8a-3fbe-46be-a7dc-c6916d3731c4",
   "metadata": {},
   "source": [
    "### Low or high fidelity image understanding\n",
    "The detail parameter—which has three options, low, high, and auto—gives you control over how the model processes the image and generates\n",
    "its textual understanding. By default, the model will use the auto setting, which looks at the image input size and decides if it should use \n",
    "the low or high setting.\n",
    "\n",
    "low enables the \"low res\" mode. The model receives a low-resolution 512px x 512px version of the image. It represents the image with a budget\n",
    "of 85 tokens. This allows the API to return faster responses and consume fewer input tokens for use cases that do not require high detail.\n",
    "high enables \"high res\" mode, which first lets the model see the low-resolution image (using 85 tokens) and then creates detailed crops using\n",
    "170 tokens for each 512px x 512px tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c012858-40b2-4999-b5b0-c48236f8ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                        \"detail\": \"high\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d99d0-c5f3-49d2-abf3-086cad4c3227",
   "metadata": {},
   "source": [
    "### Image generation\n",
    "DALL·E 2 and DALL·E 3 have different options for generating images.\n",
    "The following code example uses DALL·E 3 to generate a square, standard quality image of a cat.\n",
    "#### Size and quality options\n",
    "Square, standard quality images are the fastest to generate. The default size of generated images is 1024x1024 pixels,\n",
    "but each model has different options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030783cd-5bda-4ce6-8074-8e91eb30d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"a white siamese cat\",\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"standard\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "print(response.data[0].url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e9018-fdf9-43fc-843b-296025205211",
   "metadata": {},
   "source": [
    "### Edits (DALL·E 2 only)\n",
    "The image edits endpoint lets you edit or extend an image by uploading an image and mask indicating which areas should be replaced.\n",
    "This process is also known as inpainting.\n",
    "\n",
    "The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, not just the erased area. This endpoint enables experiences like DALL·E image editing in ChatGPT Plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef993ce-a1a5-4956-a554-11b90e893b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit an image\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.images.edit(\n",
    "    model=\"dall-e-2\",\n",
    "    image=open(\"sunlit_lounge.png\", \"rb\"),\n",
    "    mask=open(\"mask.png\", \"rb\"),\n",
    "    prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\",\n",
    "    n=1,  #means generate 1 image variation based on the provided input (image, mask, and prompt).\n",
    "    size=\"1024x1024\",\n",
    ")\n",
    "\n",
    "print(response.data[0].url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec851c3-8036-4d38-839a-2656124a14ea",
   "metadata": {},
   "source": [
    "![Alt Text](D:/Projects/DeepLearning/FineTuning/images/sunlit.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65256bb5-6c3f-4897-9810-471b46701548",
   "metadata": {},
   "source": [
    "### Variations (DALL·E 2 only)\n",
    "The image variations endpoint allows you to generate a variation of a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a3252f-9e5e-430d-805e-79703a4b70b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate an image variation\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.images.create_variation(\n",
    "    model=\"dall-e-2\",\n",
    "    image=open(\"corgi_and_cat_paw.png\", \"rb\"),\n",
    "    n=1, # means generate 1 image variation based on the provided input (image, mask, and prompt).\n",
    "    size=\"1024x1024\"\n",
    ")\n",
    "\n",
    "print(response.data[0].url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c44553-ca5d-4ac7-9fb7-c8059592ff8e",
   "metadata": {},
   "source": [
    "![Alt Text](D:/Projects/DeepLearning/FineTuning/images/dogCat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0392f8-530b-440e-9475-a8e9a0a177b0",
   "metadata": {},
   "source": [
    "### Error handling\n",
    "API requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a try...except statement, and the error details can be found in e.error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96f324-43bf-4582-a51b-74cc0e9ab11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "try:\n",
    "  response = client.images.create_variation(\n",
    "    image=open(\"image_edit_mask.png\", \"rb\"),\n",
    "    n=1,\n",
    "    model=\"dall-e-2\",\n",
    "    size=\"1024x1024\"\n",
    "  )\n",
    "  print(response.data[0].url)\n",
    "except openai.OpenAIError as e:\n",
    "  print(e.http_status) # 400 → Bad Request (wrong parameters); 401 → Unauthorized (invalid API key); 429 → Too Many Requests (rate limit exceeded)\n",
    "  print(e.error) #\"message\": \"Invalid image format\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c51d87-e521-46bd-a10e-346983dd5e39",
   "metadata": {},
   "source": [
    "## Audio generation\n",
    "\n",
    "You can use audio capabilities to:\n",
    "\n",
    "Generate a spoken audio summary of a body of text (text in, audio out)\n",
    "\n",
    "Perform sentiment analysis on a recording (audio in, text out)\n",
    "\n",
    "Async speech-to-speech interactions with a model (audio in, audio out)\n",
    "\n",
    "To generate audio or use audio as an input, use the chat completions endpoint. You can either use the REST API from the HTTP client of your choice or one of OpenAI's official SDKs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8ce80-b373-4dac-94d1-6581f02cbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Audio Output from model\n",
    "#Create a human-like audio response to a prompt\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-audio-preview\",\n",
    "    modalities=[\"text\", \"audio\"], # Request both text and audio response\n",
    "    audio={\"voice\": \"alloy\", \"format\": \"wav\"},# Use \"alloy\" voice and WAV format; Alloy is voice choice similar to human; Echo, Fable, Onyx, Nova, Shimmer\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Is a golden retriever a good family dog?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0])\n",
    "\n",
    "wav_bytes = base64.b64decode(completion.choices[0].message.audio.data)\n",
    "with open(\"dog.wav\", \"wb\") as f:\n",
    "    f.write(wav_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc412c2-f99f-425c-bf98-1b898b94fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio input to model\n",
    "#Use audio inputs for prompting a model\n",
    "import base64\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Fetch the audio file and convert it to a base64 encoded string\n",
    "url = \"https://cdn.openai.com/API/docs/audio/alloy.wav\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status() #If the request fails (e.g., 404 Not Found or 500 Server Error), it raises an error instead of continuing execution.\n",
    "wav_data = response.content \n",
    "encoded_string = base64.b64encode(wav_data).decode('utf-8') #UTF-8 → A text encoding format that supports all characters.\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-audio-preview\",\n",
    "    modalities=[\"text\", \"audio\"],\n",
    "    audio={\"voice\": \"alloy\", \"format\": \"wav\"},\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in this recording?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_audio\",\n",
    "                    \"input_audio\": {\n",
    "                        \"data\": encoded_string,\n",
    "                        \"format\": \"wav\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce19e9-ca01-4285-bead-f85ef7f27ca1",
   "metadata": {},
   "source": [
    "### Multi-turn conversations\n",
    "Using audio outputs from the model as inputs to multi-turn conversations requires a generated ID. Find this ID in the response data for an audio generation. Here's an example of a message you might receive from /chat/completions in a JSON data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0a7de-0697-448a-ab5f-0f2d2c324a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"index\": 0,\n",
    "  \"message\": {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": null, #No text response because the output is audio\n",
    "    \"refusal\": null, #The AI didn’t refuse the request.\n",
    "    \"audio\": {\n",
    "      \"id\": \"audio_abc123\",\n",
    "      \"expires_at\": 1729018505,\n",
    "      \"data\": \"<bytes omitted>\", #This is where the actual audio file is stored.\n",
    "      \"transcript\": \"Yes, golden retrievers are known to be ...\" #This is the text version of what the AI said in the audio.\n",
    "    }\n",
    "  },\n",
    "  \"finish_reason\": \"stop\" #The response was completed successfully.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c1d5c-cfc0-4145-95e2-186c879c31dc",
   "metadata": {},
   "source": [
    "The value of message.audio.id above provides an identifier you can use in an assistant message for a new /chat/completions request, as in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c9cd9-f493-44f0-9a71-8ef8ab5530d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl \"https://api.openai.com/v1/chat/completions\" \\ \n",
    "    -H \"Content-Type: application/json\" \\  #-H (Header) → Adds HTTP headers This tells the API that the request body is in JSON format.\n",
    "    -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
    "#-d (Data) → Sends the request body; This sends JSON data to the API, specifying what we want (like the model, messages, and response type).\n",
    "    -d '{ \n",
    "        \"model\": \"gpt-4o-audio-preview\",\n",
    "        \"modalities\": [\"text\", \"audio\"],\n",
    "        \"audio\": { \"voice\": \"alloy\", \"format\": \"wav\" },\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Is a golden retriever a good family dog?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"audio\": {\n",
    "                    \"id\": \"audio_abc123\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Why do you say they are loyal?\"\n",
    "            }\n",
    "        ]\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de91ff-cc12-49ed-bf08-b651932cdabc",
   "metadata": {},
   "source": [
    "### How do I think about audio input to the model in terms of tokens?\n",
    "We're working on better tooling to expose this, but roughly one hour of audio input equals 128k tokens, the max context window currently supported by this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62385ed4-e13d-4cb9-a278-3165a6dda9e0",
   "metadata": {},
   "source": [
    "## Text to speech\n",
    "The Audio API provides a speech endpoint based on our TTS (text-to-speech) model. It comes with six built-in voices and can be used to:\n",
    "- Narrate a written blog post\n",
    "- Produce spoken audio in multiple languages\n",
    "- Give realtime audio output using streaming\n",
    "- \n",
    "The speech endpoint takes three key inputs: 1)**model,** 2) **the text to be turned into audio,** and 3) **the voice you want to use in the output.** Here's a simple request example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf37eac-d2da-4b31-87ce-e17425d90155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "speech_file_path = Path(__file__).parent / \"speech.mp3\"\n",
    "response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"alloy\",\n",
    "    input=\"Today is a wonderful day to build something people love!\",\n",
    ")\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ebb6e-c54f-4beb-8387-cf8cac256553",
   "metadata": {},
   "source": [
    "By default, the endpoint outputs an MP3 of the spoken audio, but you can configure it to output any supported format.\n",
    "### Audio quality\n",
    "For realtime applications, the standard tts-1 model provides the lowest latency, but at a lower quality than the tts-1-hd model.\n",
    "\n",
    "### Voice options\n",
    "Experiment with different voices (alloy, ash, coral, echo, fable, onyx, nova, sage, shimmer) to find a match for your desired tone and audience. Current voices are optimized for English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cfb03-13b9-4c54-a832-f1205b23e763",
   "metadata": {},
   "source": [
    "### Streaming realtime audio\n",
    "The Speech API provides support for realtime audio streaming using chunk transfer encoding. This means the audio can be played before the full file is generated and made accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2395f-a730-4e53-b7da-ddbdae2eb8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"alloy\",\n",
    "    input=\"Hello world! This is a streaming test.\",\n",
    ")\n",
    "\n",
    "response.stream_to_file(\"output.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b25e6b-753d-4fd3-81b9-c6509bfe0edc",
   "metadata": {},
   "source": [
    "### Supported output formats\n",
    "The default response format is mp3, but other formats like opus and wav are available.\n",
    "\n",
    "- MP3: The default response format for general use cases.\n",
    "- Opus: For internet streaming and communication, low latency.\n",
    "- AAC: For digital audio compression, preferred by YouTube, Android, iOS.\n",
    "- FLAC: For lossless audio compression, favored by audio enthusiasts for archiving.\n",
    "- WAV: Uncompressed WAV audio, suitable for low-latency applications to avoid decoding overhead.\n",
    "- PCM: Similar to WAV but contains the raw samples in 24kHz (16-bit signed, low-endian), without the header."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac12d2fb-a45f-4f46-8267-be955783fcff",
   "metadata": {},
   "source": [
    "## Speech to text\n",
    "\n",
    "The Audio API provides two speech to text endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:\n",
    "\n",
    "- Transcribe audio into whatever language the audio is in.\n",
    "- Translate and transcribe the audio into english.\n",
    "File uploads are currently limited to 25 MB and the following input file types are supported:\n",
    "mp3, mp4, mpeg, mpga, m4a, wav, and webm.\n",
    "### Transcriptions\n",
    "The transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. We currently support multiple input and output file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc03c4-bde3-4bf9-89a8-e71aa00da35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file\n",
    ")\n",
    "\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf7bcd-18b4-44bf-b7aa-d31be7ab4fd4",
   "metadata": {},
   "source": [
    "By default, the response type will be json with the raw text included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233689f-5bbd-409f-9c02-0ff5d575c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.\n",
    "....\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ced35e-1feb-46e0-88c8-fda74aff9647",
   "metadata": {},
   "source": [
    "The Audio API also allows you to set additional parameters in a request. For example, if you want to set the response_format as text, your request would look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537dd4b-d885-4374-8c3f-3970694c903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file = open(\"/path/to/file/speech.mp3\", \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file, \n",
    "    response_format=\"text\"\n",
    ")\n",
    "\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4dc46-9735-47d7-ab7b-770260fc9dea",
   "metadata": {},
   "source": [
    "### Translations\n",
    "The translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41667aba-5174-4770-bad5-7c640cfe917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file = open(\"/path/to/file/german.mp3\", \"rb\")\n",
    "transcription = client.audio.translations.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    ")\n",
    "\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd804b-f838-4c05-85d1-306a1334714d",
   "metadata": {},
   "source": [
    "### Timestamps\n",
    "By default, the Whisper API will output a transcript of the provided audio in text. The timestamp_granularities[] parameter enables a more structured and timestamped json output format, with timestamps at the segment, word level, or both. This enables word-level precision for transcripts and video edits, which allows for the removal of specific frames tied to individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452a046-0ab5-4820-a4ae-f57d36e27282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without timestamp\n",
    "{\n",
    "  \"text\": \"The quick brown fox jumps over the lazy dog.\"\n",
    "}\n",
    "\n",
    "#with segment\n",
    "{\n",
    "  \"segments\": [\n",
    "    {\n",
    "      \"start\": 0.0,\n",
    "      \"end\": 5.0,\n",
    "      \"text\": \"The quick brown fox jumps over the lazy dog.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "#with words\n",
    "{\n",
    "  \"words\": [\n",
    "    { \"start\": 0.0, \"end\": 0.5, \"text\": \"The\" },\n",
    "    { \"start\": 0.5, \"end\": 0.9, \"text\": \"quick\" },\n",
    "    { \"start\": 0.9, \"end\": 1.2, \"text\": \"brown\" },\n",
    "    ...\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635e767-bcd9-43f8-aa90-307dd81fbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamp options\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file = open(\"/path/to/file/speech.mp3\", \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "    file=audio_file,\n",
    "    model=\"whisper-1\",\n",
    "    response_format=\"verbose_json\",\n",
    "    timestamp_granularities=[\"word\"]\n",
    ")\n",
    "\n",
    "print(transcription.words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10800419-4cdc-4a31-8857-0a7f69e2ce54",
   "metadata": {},
   "source": [
    "### Longer inputs\n",
    "By default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.\n",
    "\n",
    "One way to handle this is to use the **PyDub** open source Python package to split the audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30af10-55dc-49c5-819a-e63d44de77bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "song = AudioSegment.from_mp3(\"good_morning.mp3\")\n",
    "\n",
    "#pydub handle time in miliseconds\n",
    "ten_minutes = 10 * 60 * 1000\n",
    "first_10_minutes = song[:ten_minutes]\n",
    "first_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
