{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39635fc4-a8e0-4dbe-9588-c4b543195397",
   "metadata": {},
   "source": [
    "### Moderation\n",
    "Identify potentially harmful content in text and images.\n",
    "Use the moderations endpoint to check whether text or images are potentially harmful. If harmful content is identified, you can take corrective action, like filtering content or intervening with user accounts creating offending content. The moderation endpoint is free to use.\n",
    "\n",
    "You can use two models for this endpoint:\n",
    "\n",
    "__omni-moderation-latest:__ This model and all snapshots support more categorization options and multi-modal inputs.\n",
    "\n",
    "__text-moderation-latest (Legacy):__ Older model that supports only text inputs and fewer input categorizations. The newer omni-moderation models will be the best choice for new applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e1034-cd7d-4bb4-85ba-7417887d0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get classification information for a text input\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.moderations.create(\n",
    "    model=\"omni-moderation-latest\",\n",
    "    input=\"...text to classify goes here...\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3951929-1c98-41e7-bfd2-8217c86e50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification information for image and text input\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.moderations.create(\n",
    "    model=\"omni-moderation-latest\",\n",
    "    input=[\n",
    "        {\"type\": \"text\", \"text\": \"...text to classify goes here...\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": \"https://example.com/image.png\",\n",
    "                # can also use base64 encoded image URLs\n",
    "                # \"url\": \"data:image/jpeg;base64,abcdefg...\"\n",
    "            }\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5a935-74ac-4fb4-ba10-ee79d7c7ebd9",
   "metadata": {},
   "source": [
    "Here's a full example output, where the input is an image from a single frame of a war movie. The model correctly predicts indicators of violence in the image, with a violence category score of greater than 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73adc5bd-7672-4d20-a5ee-f9be86373dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"id\": \"modr-970d409ef3bef3b70c73d8232df86e7d\",\n",
    "  \"model\": \"omni-moderation-latest\",\n",
    "  \"results\": [\n",
    "    {\n",
    "      \"flagged\": true,\n",
    "      \"categories\": {\n",
    "        \"sexual\": false,\n",
    "        \"sexual/minors\": false,\n",
    "        \"harassment\": false,\n",
    "        \"harassment/threatening\": false,\n",
    "        \"hate\": false,\n",
    "        \"hate/threatening\": false,\n",
    "        \"illicit\": false,\n",
    "        \"illicit/violent\": false,\n",
    "        \"self-harm\": false,\n",
    "        \"self-harm/intent\": false,\n",
    "        \"self-harm/instructions\": false,\n",
    "        \"violence\": true,\n",
    "        \"violence/graphic\": false\n",
    "      },\n",
    "      \"category_scores\": {\n",
    "        \"sexual\": 2.34135824776394e-7,\n",
    "        \"sexual/minors\": 1.6346470245419304e-7,\n",
    "        \"harassment\": 0.0011643905680426018,\n",
    "        \"harassment/threatening\": 0.0022121340080906377,\n",
    "        \"hate\": 3.1999824407395835e-7,\n",
    "        \"hate/threatening\": 2.4923252458203563e-7,\n",
    "        \"illicit\": 0.0005227032493135171,\n",
    "        \"illicit/violent\": 3.682979260160596e-7,\n",
    "        \"self-harm\": 0.0011175734280627694,\n",
    "        \"self-harm/intent\": 0.0006264858507989037,\n",
    "        \"self-harm/instructions\": 7.368592981140821e-8,\n",
    "        \"violence\": 0.8599265510337075,\n",
    "        \"violence/graphic\": 0.37701736389561064\n",
    "      },\n",
    "      \"category_applied_input_types\": {\n",
    "        \"sexual\": [\n",
    "          \"image\"\n",
    "        ],\n",
    "        \"sexual/minors\": [],\n",
    "        \"harassment\": [],\n",
    "        \"harassment/threatening\": [],\n",
    "        \"hate\": [],\n",
    "        \"hate/threatening\": [],\n",
    "        \"illicit\": [],\n",
    "        \"illicit/violent\": [],\n",
    "        \"self-harm\": [\n",
    "          \"image\"\n",
    "        ],\n",
    "        \"self-harm/intent\": [\n",
    "          \"image\"\n",
    "        ],\n",
    "        \"self-harm/instructions\": [\n",
    "          \"image\"\n",
    "        ],\n",
    "        \"violence\": [\n",
    "          \"image\"\n",
    "        ],\n",
    "        \"violence/graphic\": [\n",
    "          \"image\"\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72373212-fc63-4c1b-97bc-21b7171442f5",
   "metadata": {},
   "source": [
    "### Reasoning models\n",
    "Explore advanced reasoning and problem-solving models.\n",
    "Reasoning models, like OpenAI __o1__ and __o3-mini,__ are new large language models trained with reinforcement learning to perform complex reasoning. Reasoning models think before they answer, producing a long internal chain of thought before responding to the user. Reasoning models excel in complex problem solving, coding, scientific reasoning, and multi-step planning for agentic workflows.\n",
    "\n",
    "As with our GPT models, we provide both a smaller, faster model (o3-mini) that is less expensive per token, and a larger model (o1) that is somewhat slower and more expensive, but can often generate better responses for complex tasks, and generalize better across domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972bf9d8-2006-41d8-805b-f9dc348e87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a reasoning model in chat completions\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write a bash script that takes a matrix represented as a string with \n",
    "format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d72583-ec47-435d-b4a6-c38f87127793",
   "metadata": {},
   "source": [
    "#### Reasoning effort\n",
    "In the examples above, the reasoning_effort parameter (lovingly referred to as the \"juice\" during the development of these models) is used to give the model guidance on how many reasoning tokens it should generate before creating a response to the prompt. You can specify one of low, medium, or high for this parameter, where low will favor speed and economical token usage, and high will favor more complete reasoning at the cost of more tokens generated and slower responses. The default value is medium, which is a balance between speed and reasoning accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccaf20b-a552-4c35-a5f9-cef537be5fa8",
   "metadata": {},
   "source": [
    "### Managing the context window\n",
    "It's important to ensure there's enough space in the context window for reasoning tokens when creating completions. Depending on the problem's complexity, the models may generate anywhere from a few hundred to tens of thousands of reasoning tokens. The exact number of reasoning tokens used is visible in the usage object of the chat completion response object, under completion_tokens_details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2efd5-4e7c-407f-88fa-99e9cdac92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 9,\n",
    "    \"completion_tokens\": 12,\n",
    "    \"total_tokens\": 21,\n",
    "    \"completion_tokens_details\": {\n",
    "      \"reasoning_tokens\": 0,\n",
    "      \"accepted_prediction_tokens\": 0,\n",
    "      \"rejected_prediction_tokens\": 0\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131bddf-b5f8-4a9f-ab31-72360f1bfe4d",
   "metadata": {},
   "source": [
    "#### Coding (refactoring)\n",
    "OpenAI o-series models are able to implement complex algorithms and produce code. This prompt asks o1 to refactor a React component based on some specific criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866a28a-27f9-43e4-a0ac-9f1ccf953f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refactor code\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Instructions:\n",
    "- Given the React component below, change it so that nonfiction books have red\n",
    "  text. \n",
    "- Return only the code in your reply\n",
    "- Do not include any additional formatting, such as markdown code blocks\n",
    "- For formatting, use four space tabs, and do not allow any lines of code to \n",
    "  exceed 80 columns\n",
    "\n",
    "const books = [\n",
    "  { title: 'Dune', category: 'fiction', id: 1 },\n",
    "  { title: 'Frankenstein', category: 'fiction', id: 2 },\n",
    "  { title: 'Moneyball', category: 'nonfiction', id: 3 },\n",
    "];\n",
    "\n",
    "export default function BookList() {\n",
    "  const listItems = books.map(book =>\n",
    "    <li>\n",
    "      {book.title}\n",
    "    </li>\n",
    "  );\n",
    "\n",
    "  return (\n",
    "    <ul>{listItems}</ul>\n",
    "  );\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac46213-8f40-4bd0-907c-59f5c108bd7f",
   "metadata": {},
   "source": [
    "#### Coding (Planning)\n",
    "OpenAI o-series models are also adept in creating multi-step plans. This example prompt asks o1 to create a filesystem structure for a full solution, along with Python code that implements the desired use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2788fe7-79f7-481a-abd9-301828c6a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plan and create a Python project\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"\n",
    "I want to build a Python app that takes user questions and looks \n",
    "them up in a database where they are mapped to answers. If there \n",
    "is close match, it retrieves the matched answer. If there isn't, \n",
    "it asks the user to provide an answer and stores the \n",
    "question/answer pair in the database. Make a plan for the directory \n",
    "structure you'll need, then return each file in full. Only supply \n",
    "your reasoning at the beginning and end, not throughout the code.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad25be6-79d1-4b94-ab62-48999bfd8b0c",
   "metadata": {},
   "source": [
    "### STEM Research\n",
    "STEM research refers to scientific studies and investigations in the fields of Science, Technology, Engineering, and Mathematics (STEM). \n",
    "OpenAI o-series models have shown excellent performance in STEM research. Prompts asking for support of basic research tasks should show strong results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c30a38-f3ec-403e-80fe-9d049dae13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ask questions related to basic scientific research\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"\n",
    "What are three compounds we should consider investigating to \n",
    "advance research into new antibiotics? Why should we consider \n",
    "them?\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b5702-fdec-41bc-ae68-d87b0148e0af",
   "metadata": {},
   "source": [
    "### Structured Outputs\n",
    "Ensure responses adhere to a JSON schema.\n",
    "#### Advantages of pydantic\n",
    "- Type validation: If you try participants=123, it will raise an error.\n",
    "- Auto-formatting: It ensures the data is consistent.\n",
    "- Integration with APIs & AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62403382-5a6c-46ae-9a55-3395e043eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel #Pydantic is a Python library that helps you validate and structure data using Python classes. \n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9673ee-66ef-4b6e-81a5-446eef6fc8c6",
   "metadata": {},
   "source": [
    "#### Supported models\n",
    "Structured Outputs is available in our latest large language models, starting with GPT-4o:\n",
    "\n",
    "- gpt-4.5-preview-2025-02-27 and later\n",
    "- o3-mini-2025-1-31 and later\n",
    "- o1-2024-12-17 and later\n",
    "- gpt-4o-mini-2024-07-18 and later\n",
    "- gpt-4o-2024-08-06 and later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5402d2c-1c27-4a33-8faf-8972d646dd07",
   "metadata": {},
   "source": [
    "#### When to use Structured Outputs via function calling vs via response_format\n",
    "- ***When using function calling***\n",
    "- ***When using a json_schema response format***\n",
    "\n",
    "If you are connecting the model to tools, functions, data, etc. in your system, then you should use ***function calling***\n",
    "\n",
    "If you want to structure the model's output when it responds to the user, then you should use a structured ***response_format***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed92a25-a6b3-4c0f-af7a-41ae21bc615e",
   "metadata": {},
   "source": [
    "### Chain of Thought\n",
    "You can ask the model to output an answer in a structured, step-by-step way, to guide the user through the solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac075a-0c63-4ec5-854c-b86a9587cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Structured Outputs for chain-of-thought math tutoring\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step] # steps: list[Step] means that steps is a list where each item is of type Step; Step is another Pydantic model\n",
    "    final_answer: str\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n",
    "    ],\n",
    "    response_format=MathReasoning,\n",
    ")\n",
    "\n",
    "math_reasoning = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c68f2-fb7c-4ebb-810f-b8f92e70e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"steps\": [\n",
    "    {\n",
    "      \"explanation\": \"Start with the equation 8x + 7 = -23.\",\n",
    "      \"output\": \"8x + 7 = -23\"\n",
    "    },\n",
    "    {\n",
    "      \"explanation\": \"Subtract 7 from both sides to isolate the term with the variable.\",\n",
    "      \"output\": \"8x = -23 - 7\"\n",
    "    },\n",
    "    {\n",
    "      \"explanation\": \"Simplify the right side of the equation.\",\n",
    "      \"output\": \"8x = -30\"\n",
    "    },\n",
    "    {\n",
    "      \"explanation\": \"Divide both sides by 8 to solve for x.\",\n",
    "      \"output\": \"x = -30 / 8\"\n",
    "    },\n",
    "    {\n",
    "      \"explanation\": \"Simplify the fraction.\",\n",
    "      \"output\": \"x = -15 / 4\"\n",
    "    }\n",
    "  ],\n",
    "  \"final_answer\": \"x = -15 / 4\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65038f51-e854-4d37-a99f-b0dd7534c823",
   "metadata": {},
   "source": [
    "#### Structured data extraction\n",
    "You can define structured fields to extract from unstructured input data, such as research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48973656-fe24-40f5-b10b-fe91c5c4f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class ResearchPaperExtraction(BaseModel):\n",
    "    title: str\n",
    "    authors: list[str]\n",
    "    abstract: str\n",
    "    keywords: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.\"},\n",
    "        {\"role\": \"user\", \"content\": \"...\"}\n",
    "    ],\n",
    "    response_format=ResearchPaperExtraction,\n",
    ")\n",
    "\n",
    "research_paper = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc75ae9-0c84-478e-9b74-e3feeaf0efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"title\": \"Application of Quantum Algorithms in Interstellar Navigation: A New Frontier\",\n",
    "  \"authors\": [\n",
    "    \"Dr. Stella Voyager\",\n",
    "    \"Dr. Nova Star\",\n",
    "    \"Dr. Lyra Hunter\"\n",
    "  ],\n",
    "  \"abstract\": \"This paper investigates the utilization of quantum algorithms to improve interstellar navigation systems. By leveraging quantum superposition and entanglement, our proposed navigation system can calculate optimal travel paths through space-time anomalies more efficiently than classical methods. Experimental simulations suggest a significant reduction in travel time and fuel consumption for interstellar missions.\",\n",
    "  \"keywords\": [\n",
    "    \"Quantum algorithms\",\n",
    "    \"interstellar navigation\",\n",
    "    \"space-time anomalies\",\n",
    "    \"quantum superposition\",\n",
    "    \"quantum entanglement\",\n",
    "    \"space travel\"\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75475d8-6bba-48dc-b7bb-f30d0aa237fb",
   "metadata": {},
   "source": [
    "#### UI Generation\n",
    "You can generate valid HTML by representing it as recursive data structures with constraints, like enums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b3ca3-c949-4096-a519-ef3a728a42b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating HTML using Structured Outputs\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class UIType(str, Enum):\n",
    "    div = \"div\"\n",
    "    button = \"button\"\n",
    "    header = \"header\"\n",
    "    section = \"section\"\n",
    "    field = \"field\"\n",
    "    form = \"form\"\n",
    "\n",
    "class Attribute(BaseModel):\n",
    "    name: str\n",
    "    value: str\n",
    "\n",
    "class UI(BaseModel):\n",
    "    type: UIType\n",
    "    label: str\n",
    "    children: List[\"UI\"] \n",
    "    attributes: List[Attribute]\n",
    "\n",
    "UI.model_rebuild() # This is required to enable recursive types\n",
    "\n",
    "class Response(BaseModel):\n",
    "    ui: UI\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a UI generator AI. Convert the user input into a UI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Make a User Profile Form\"}\n",
    "    ],\n",
    "    response_format=Response,\n",
    ")\n",
    "\n",
    "ui = completion.choices[0].message.parsed\n",
    "print(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4cfe28-350d-442a-aafc-7a02675a48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"type\": \"form\",\n",
    "    \"label\": \"User Profile Form\",\n",
    "    \"children\": [\n",
    "        {\n",
    "            \"type\": \"div\",\n",
    "            \"label\": \"\",\n",
    "            \"children\": [\n",
    "                {\n",
    "                    \"type\": \"field\",\n",
    "                    \"label\": \"First Name\",\n",
    "                    \"children\": [],\n",
    "                    \"attributes\": [\n",
    "                        {\n",
    "                            \"name\": \"type\",\n",
    "                            \"value\": \"text\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"name\",\n",
    "                            \"value\": \"firstName\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"placeholder\",\n",
    "                            \"value\": \"Enter your first name\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"field\",\n",
    "                    \"label\": \"Last Name\",\n",
    "                    \"children\": [],\n",
    "                    \"attributes\": [\n",
    "                        {\n",
    "                            \"name\": \"type\",\n",
    "                            \"value\": \"text\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"name\",\n",
    "                            \"value\": \"lastName\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"placeholder\",\n",
    "                            \"value\": \"Enter your last name\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"attributes\": []\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"button\",\n",
    "            \"label\": \"Submit\",\n",
    "            \"children\": [],\n",
    "            \"attributes\": [\n",
    "                {\n",
    "                    \"name\": \"type\",\n",
    "                    \"value\": \"submit\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"attributes\": [\n",
    "        {\n",
    "            \"name\": \"method\",\n",
    "            \"value\": \"post\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"action\",\n",
    "            \"value\": \"/submit-profile\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b9b6c4-42e8-405d-9c65-0f4cb92b4369",
   "metadata": {},
   "source": [
    "#### Moderation\n",
    "You can classify inputs on multiple categories, which is a common way of doing moderation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc45354-00c0-4128-a49e-26df6ea5dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moderation using Structured Outputs\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Category(str, Enum):\n",
    "    violence = \"violence\"\n",
    "    sexual = \"sexual\"\n",
    "    self_harm = \"self_harm\"\n",
    "\n",
    "class ContentCompliance(BaseModel):\n",
    "    is_violating: bool\n",
    "    category: Optional[Category]\n",
    "    explanation_if_violating: Optional[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Determine if the user input violates specific guidelines and explain if they do.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How do I prepare for a job interview?\"}\n",
    "    ],\n",
    "    response_format=ContentCompliance,\n",
    ")\n",
    "\n",
    "compliance = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ec77c-6cc1-45a1-85bb-a0cc2b2266a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"is_violating\": false,\n",
    "  \"category\": null,\n",
    "  \"explanation_if_violating\": null\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa3e739-345f-4133-8693-88bf4adb0537",
   "metadata": {},
   "source": [
    "### How to use Structured Outputs with response_format\n",
    "You can use Structured Outputs with the new SDK helper to parse the model's output into your desired format, or you can specify the JSON schema directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6d72c4-6c5e-4d04-929f-fc522378347b",
   "metadata": {},
   "source": [
    "#### Step 1: Define your object\n",
    "First you must define an object or data structure to represent the JSON Schema that the model should be constrained to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9736092-f72a-410f-9c62-a7c735c83cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathResponse(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09862361-6fd8-45ef-8486-d81798a75151",
   "metadata": {},
   "source": [
    "#### Step 2: Supply your object in the API call\n",
    "You can use the parse method to automatically parse the JSON response into the object you defined.\n",
    "\n",
    "Under the hood, the SDK takes care of supplying the JSON schema corresponding to your data structure, and then parsing the response as an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb3529-d1a5-4aaf-a127-333b32ca1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.beta.chat.completion.create(\n",
    "    model = \"gpt-4o-2024-08-06\"\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How can I solve 8x + 7 = -23\"}\n",
    "],\n",
    "respone_format = MathResponse\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17ac6b-f424-4719-aab5-5d049be5763f",
   "metadata": {},
   "source": [
    "#### Step 3: Handle edge cases\n",
    "In some cases, the model might not generate a valid response that matches the provided JSON schema.\n",
    "\n",
    "This can happen in the case of a refusal, if the model refuses to answer for safety reasons, or if for example you reach a max tokens limit and the response is incomplete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d89a3-4df2-4952-b0af-1fb6c1961cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"},\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"math_response\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"steps\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"explanation\": {\"type\": \"string\"},\n",
    "                                    \"output\": {\"type\": \"string\"},\n",
    "                                },\n",
    "                                \"required\": [\"explanation\", \"output\"],\n",
    "                                \"additionalProperties\": False,\n",
    "                            },\n",
    "                        },\n",
    "                        \"final_answer\": {\"type\": \"string\"},\n",
    "                    },\n",
    "                    \"required\": [\"steps\", \"final_answer\"],\n",
    "                    \"additionalProperties\": False,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        strict=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    # handle errors like finish_reason, refusal, content_filter, etc.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7921e90-be0b-4667-86e7-69b34b11527b",
   "metadata": {},
   "source": [
    "#### Refusals with Structured Outputs\n",
    "When using Structured Outputs with user-generated input, OpenAI models may occasionally refuse to fulfill the request for safety reasons. Since a refusal does not necessarily follow the schema you have supplied in response_format, the API response will include a new field called __refusal__ to indicate that the model refused to fulfill the request.\n",
    "\n",
    "When the refusal property appears in your output object, you might present the refusal in your UI, or include conditional logic in code that consumes the response to handle the case of a refused request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c36465-3e58-444d-8147-e1f01824e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n",
    "    ],\n",
    "    response_format=MathReasoning,\n",
    ")\n",
    "\n",
    "math_reasoning = completion.choices[0].message\n",
    "\n",
    "# If the model refuses to respond, you will get a refusal message\n",
    "if (math_reasoning.refusal):\n",
    "    print(math_reasoning.refusal)\n",
    "else:\n",
    "    print(math_reasoning.parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69b60c-d5b4-4512-bf9a-939690f7cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"id\": \"chatcmpl-9nYAG9LPNonX8DAyrkwYfemr3C8HC\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1721596428,\n",
    "  \"model\": \"gpt-4o-2024-08-06\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"refusal\": \"I'm sorry, I cannot assist with that request.\" #update code\n",
    "      },\n",
    "      \"logprobs\": null,\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 81,\n",
    "    \"completion_tokens\": 11,\n",
    "    \"total_tokens\": 92,\n",
    "    \"completion_tokens_details\": {\n",
    "      \"reasoning_tokens\": 0,\n",
    "      \"accepted_prediction_tokens\": 0,\n",
    "      \"rejected_prediction_tokens\": 0\n",
    "    }\n",
    "  },\n",
    "  \"system_fingerprint\": \"fp_3407719c7f\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97fff8-9efd-439f-9554-5fb6736a3e85",
   "metadata": {},
   "source": [
    "#### Streaming\n",
    "You can use streaming to process model responses or function call arguments as they are being generated, and parse them as structured data.\n",
    "\n",
    "That way, you don't have to wait for the entire response to complete before handling it. This is particularly useful if you would like to display JSON fields one by one, or handle function call arguments as soon as they are available.\n",
    "\n",
    "We recommend relying on the SDKs to handle streaming with Structured Outputs. You can find an example of how to stream function call arguments without the SDK stream helper in the function calling guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd585d2d-e485-468a-9724-20391b431698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "class EntitiesModel(BaseModel):\n",
    "    attributes: List[str]\n",
    "    colors: List[str]\n",
    "    animals: List[str]\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "with client.beta.chat.completions.stream(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract entities from the input text\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The quick brown fox jumps over the lazy dog with piercing blue eyes\",\n",
    "        },\n",
    "    ],\n",
    "    response_format=EntitiesModel,\n",
    ") as stream: # stream  is a special kind of object that continuously yields data as the model generates tokens.\n",
    "    for event in stream:\n",
    "        if event.type == \"content.delta\": #means this event contains part of the response (a small chunk of words).\n",
    "            if event.parsed is not None:\n",
    "                # Print the parsed data as JSON\n",
    "                print(\"content.delta parsed:\", event.parsed)\n",
    "        elif event.type == \"content.done\": #means the model has finished generating the response.\n",
    "            print(\"content.done\")\n",
    "        elif event.type == \"error\":\n",
    "            print(\"Error in stream:\", event.error)\n",
    "\n",
    "final_completion = stream.get_final_completion()\n",
    "print(\"Final completion:\", final_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c6c1c-6ab6-4731-991d-07c47cc3a405",
   "metadata": {},
   "source": [
    "You can also use the stream helper to parse function call arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076866c2-171c-4fed-b759-851ad2faa87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "with client.beta.chat.completions.stream(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather like in SF and London?\",\n",
    "        },\n",
    "    ],\n",
    "    tools=[\n",
    "        openai.pydantic_function_tool(GetWeather, name=\"get_weather\"),\n",
    "    ],\n",
    "    parallel_tool_calls=True,\n",
    ") as stream:\n",
    "    for event in stream:\n",
    "        if event.type == \"tool_calls.function.arguments.delta\" or event.type == \"tool_calls.function.arguments.done\":\n",
    "            print(event)\n",
    "\n",
    "print(stream.get_final_completion())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323036f-1c27-4328-8072-403ce18fc658",
   "metadata": {},
   "source": [
    "### Predicted Outputs\n",
    "Reduce latency for model responses where much of the response is known ahead of time.\n",
    "Predicted Outputs enable you to speed up API responses from Chat Completions when many of the output tokens are known ahead of time. This is most common when you are regenerating a text or code file with minor modifications. You can provide your prediction using the prediction request parameter in Chat Completions.\n",
    "\n",
    "Predicted Outputs are available today using the __latest gpt-4o and gpt-4o-mini__ models. Read on to learn how to use Predicted Outputs to reduce latency in your applicatons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c194b-bcfc-4b31-9d57-6fb99b3c62b2",
   "metadata": {},
   "source": [
    "#### Code refactoring example\n",
    "Predicted Outputs are particularly useful for regenerating text documents and code files with small modifications. Let's say you want the GPT-4o model to refactor a piece of TypeScript code, and convert the username property of the User class to be email instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a260f0-c03d-4f20-8f3c-f759b8dd9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User {\n",
    "    firstName: string = \"\";\n",
    "    lastName: string = \"\";\n",
    "    username: string = \"\";\n",
    "}\n",
    "\n",
    "export default User;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d3653-2dae-4854-b363-ff5a254816f8",
   "metadata": {},
   "source": [
    "Most of the file will be unchanged, except for line 4 above. If you use the current text of the code file as your prediction, you can regenerate the entire file with lower latency. These time savings add up quickly for larger files.\n",
    "\n",
    "Below is an example of using the prediction parameter in our SDKs to predict that the final output of the model will be very similar to our original code file, which we use as the prediction text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25996b9c-6078-432a-9019-4fb1e1d813dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "code = \"\"\"\n",
    "class User {\n",
    "  firstName: string = \"\";\n",
    "  lastName: string = \"\";\n",
    "  username: string = \"\";\n",
    "}\n",
    "\n",
    "export default User;\n",
    "\"\"\"\n",
    "\n",
    "refactor_prompt = \"\"\"\n",
    "Replace the \"username\" property with an \"email\" property. Respond only \n",
    "with code, and with no markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": refactor_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": code\n",
    "        }\n",
    "    ],\n",
    "    prediction={\n",
    "        \"type\": \"content\",\n",
    "        \"content\": code\n",
    "    }\n",
    ")\n",
    "\n",
    "print(completion)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d28856-83e5-4718-8e10-d2d7b38c37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicted Outputs with streaming\n",
    "from openai import OpenAI\n",
    "\n",
    "code = \"\"\"\n",
    "class User {\n",
    "  firstName: string = \"\";\n",
    "  lastName: string = \"\";\n",
    "  username: string = \"\";\n",
    "}\n",
    "\n",
    "export default User;\n",
    "\"\"\"\n",
    "\n",
    "refactor_prompt = \"\"\"\n",
    "Replace the \"username\" property with an \"email\" property. Respond only \n",
    "with code, and with no markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": refactor_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": code\n",
    "        }\n",
    "    ],\n",
    "    prediction={\n",
    "        \"type\": \"content\",\n",
    "        \"content\": code\n",
    "    },\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
