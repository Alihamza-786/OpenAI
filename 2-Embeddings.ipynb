{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0359d6-e78d-465e-9f53-65315fd5903a",
   "metadata": {},
   "source": [
    "## Vector embeddings\n",
    "Learn how to turn text into numbers, unlocking use cases like search. OpenAI’s text embeddings measure the relatedness of text strings. \n",
    "- Search (where results are ranked by relevance to a query string)\n",
    "- Clustering (where text strings are grouped by similarity)\n",
    "- Recommendations (where items with related text strings are recommended)\n",
    "- Anomaly detection (where outliers with little relatedness are identified)\n",
    "- Diversity measurement (where similarity distributions are analyzed)\n",
    "- Classification (where text strings are classified by their most similar label)\n",
    "\n",
    "An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e441451-7fa2-4528-afaa-7be0a5a41609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"Your text string goes here\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "print(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519def5-316d-4057-93b6-efede45f934b",
   "metadata": {},
   "source": [
    "The response contains the embedding vector (list of floating point numbers) along with some additional metadata. You can extract the embedding vector, save it in a vector database, and use for many different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a33c6d-ff40-4105-954a-86daabeff486",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"object\": \"list\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"object\": \"embedding\",\n",
    "      \"index\": 0,\n",
    "      \"embedding\": [\n",
    "        -0.006929283495992422,\n",
    "        -0.005336422007530928,\n",
    "        -4.547132266452536e-05,\n",
    "        -0.024047505110502243\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  \"model\": \"text-embedding-3-small\",\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 5,\n",
    "    \"total_tokens\": 5\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6dbaa8-6e4f-45aa-9f57-5827f2db6521",
   "metadata": {},
   "source": [
    "### Example\n",
    "Let’s say we embed the words:\n",
    "\n",
    "- \"king\"\n",
    "- \"queen\"\n",
    "- \"apple\"\n",
    "  \n",
    "The model might map them like this in a simplified 3D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03905095-7156-45db-b8db-6b4c96b7da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"king\"   → [0.8, 1.2, -0.3]  \n",
    "\"queen\"  → [0.75, 1.3, -0.28]  # Very close to \"king\"\n",
    "\"apple\"  → [0.1, -0.5, 0.9]   # Very different from \"king\" and \"queen\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe79c9d-e9da-40fb-9430-888ef5e0aabf",
   "metadata": {},
   "source": [
    "By default, the length of the embedding vector is **1536** for __text-embedding-3-small__ or **3072** for __text-embedding-3-large. To reduce the embedding's dimensions without losing its concept-representing properties, pass in the dimensions parameter.\n",
    "\n",
    "### Obtaining the embeddings\n",
    "The dataset contains a total of 568,454 food reviews left by Amazon users up to October 2012. We use a subset of the 1000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text).\n",
    "Below, we combine the review summary and review text into a single combined text. The model encodes this combined text and output a single vector embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5fd366-dfc0-47a5-90d4-ae4b02dae612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output/embedded_1k_reviews.csv')\n",
    "df['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a72f5-c87d-40b6-b7e7-78040a7e2dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "df['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-3-small')) #lambda is anonymous function\n",
    "df.to_csv('output/embedded_1k_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dce4ee-c89d-47bb-8596-64d8b6b70006",
   "metadata": {},
   "source": [
    "### Reducing embedding dimensions\n",
    "A text-embedding-3-large embedding can be shortened to a size of __256__ while still outperforming an unshortened text-embedding-ada-002 embedding with a size of __1536.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa85f1-3358-4bf7-921b-71496c295216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae607cb-52dc-4ffe-869b-9326cb9d6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def normalize_l2(x): #This function ensures that each vector has unit length (L2 norm = 1), making it useful for similarity comparisons like cosine similarity.\n",
    "    x = np.array(x)\n",
    "    if x.ndim == 1:              #Handle 1D array\n",
    "        norm = np.linalg.norm(x) #Computes the L2 norm (Euclidean norm) using np.linalg.norm(x).\n",
    "        if norm == 0:            #If the norm is 0 (to avoid division by zero), it returns x unchanged.\n",
    "            return x\n",
    "        return x / norm          #Otherwise, it divides each element of x by the norm, ensuring the vector has a length of 1.\n",
    "    else:\n",
    "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True) #Handle 2D arrays (batch of embeddings)\n",
    "        return np.where(norm == 0, x, x / norm) #If any row has a norm of 0, it stays the same. Otherwise, each row is divided by its L2 norm.\n",
    "\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\", input=\"Testing 123\", encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "cut_dim = response.data[0].embedding[:256]\n",
    "norm_dim = normalize_l2(cut_dim)\n",
    "\n",
    "print(norm_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755143ee-77db-4d8c-8a6e-8c492cc7dd59",
   "metadata": {},
   "source": [
    "### Question answering using Embeddings\n",
    "There are many common cases where the model is not trained on data which contains key facts and information you want to make accessible when generating responses to a user query. One way of solving this, as shown below, is to put additional information into the context window of the model. This is effective in many use cases but leads to higher token costs. In this notebook, we explore the tradeoff between this approach and embeddings bases search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129bf8d-8e41-4656-81cd-65c1d0c59a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"Use the below article on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found, write \"I don't know.\"\n",
    "\n",
    "Article:\n",
    "\\\"\\\"\\\"\n",
    "{wikipedia_article_on_curling}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Question: Which athletes won the gold medal in curling at the 2022 Winter Olympics?\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You answer questions about the 2022 Winter Olympics.'},\n",
    "        {'role': 'user', 'content': query},\n",
    "    ],\n",
    "    model=GPT_MODEL,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de4952-8ed2-4f33-a53b-b6a97e215403",
   "metadata": {},
   "source": [
    "### Text search using Embeddings\n",
    "To retrieve the most relevant documents we use the cosine similarity between the embedding vectors of the query and each document, and return the highest scored documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741401b2-fd68-4562-a6de-e56bc34d66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "def search_reviews(df, product_description, n=3, pprint=True):\n",
    "    embedding = get_embedding(product_description, model='text-embedding-3-small')\n",
    "    df['similarities'] = df.ada_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "    res = df.sort_values('similarities', ascending=False).head(n)\n",
    "    return res\n",
    "\n",
    "res = search_reviews(df, 'delicious beans', n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390dfa3-9b95-4a75-8fc6-fa0150c77911",
   "metadata": {},
   "source": [
    "### Code Search Using Embeddings\n",
    "Code search works similarly to embedding-based text search. We provide a method to extract Python functions from all the Python files in a given repository. Each function is then indexed by the text-embedding-3-small model.\n",
    "\n",
    "To perform a code search, we embed the query in natural language using the same model. Then we calculate cosine similarity between the resulting query embedding and each of the function embeddings. The highest cosine similarity results are most relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727598e-c808-4c45-91fd-01f2bf595d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "df['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n",
    "\n",
    "def search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n",
    "    embedding = get_embedding(code_query, model='text-embedding-3-small')\n",
    "    df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = df.sort_values('similarities', ascending=False).head(n)\n",
    "    return res\n",
    "\n",
    "res = search_functions(df, 'Completions API tests', n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d018b-4d2d-4ca3-afdd-c900b708f784",
   "metadata": {},
   "source": [
    "### Recommendations Using Embeddings\n",
    "Because shorter distances between embedding vectors represent greater similarity, embeddings can be useful for recommendation.\n",
    "\n",
    "Below, we illustrate a basic recommender. It takes in a list of strings and one 'source' string, computes their embeddings, and then returns a ranking of the strings, ranked from most similar to least similar. As a concrete example, the linked notebook below applies a version of this function to the AG news dataset (sampled down to 2,000 news article descriptions) to return the top 5 most similar articles to any given source article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938c293-b21f-4374-8a35-e046902664cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations_from_strings(\n",
    "    strings: List[str],\n",
    "    index_of_source_string: int,\n",
    "    model=\"text-embedding-3-small\",\n",
    ") -> List[int]:\n",
    "    \"\"\"Return nearest neighbors of a given string.\"\"\"\n",
    "\n",
    "    # get embeddings for all strings\n",
    "    embeddings = [embedding_from_string(string, model=model) for string in strings]\n",
    "\n",
    "    # get the embedding of the source string\n",
    "    query_embedding = embeddings[index_of_source_string]\n",
    "\n",
    "    # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n",
    "    distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n",
    "\n",
    "    # get indices of nearest neighbors (function from embeddings_utils.py)\n",
    "    indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n",
    "    return indices_of_nearest_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c49c2c-c701-4f81-8bbb-49dc4dd7dd6c",
   "metadata": {},
   "source": [
    "### Visual Embeddings in 2D\n",
    "The size of the embeddings varies with the complexity of the underlying model. In order to visualize this high dimensional data we use the __t-SNE__ algorithm to transform the data into two dimensions.\n",
    "\n",
    "We color the individual reviews based on the star rating which the reviewer has given:\n",
    "\n",
    "1-star: red\n",
    "2-star: dark orange\n",
    "3-star: gold\n",
    "4-star: turquoise\n",
    "5-star: dark green\n",
    "#### t-SNE (t-Distributed Stochastic Neighbor Embedding) Algorithm\n",
    "t-SNE is a dimensionality reduction technique used for visualizing high-dimensional data in a lower-dimensional space (usually 2D or 3D). It helps in understanding patterns, relationships, and clusters in complex datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878950f1-ab61-44b9-9820-e9d11a746926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "df = pd.read_csv('output/embedded_1k_reviews.csv')\n",
    "matrix = df.ada_embedding.apply(eval).to_list()\n",
    "\n",
    "# Create a t-SNE model and transform the data\n",
    "tsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\n",
    "#Reduces data to 2D for visualization. If 3, it gives a 3D output. Controls how t-SNE balances local vs. global relationships. \n",
    "#Lower values focus more on local structure; higher values capture broader patterns. Usually between 5-50.\n",
    "#Ensures reproducibility—same results every time.\n",
    "#Initializes points randomly. Alternatives: 'pca' (better for structured data).    \n",
    "vis_dims = tsne.fit_transform(matrix)\n",
    "\n",
    "colors = [\"red\", \"darkorange\", \"gold\", \"turquiose\", \"darkgreen\"]\n",
    "x = [x for x,y in vis_dims]\n",
    "y = [y for x,y in vis_dims]\n",
    "color_indices = df.Score.values - 1\n",
    "\n",
    "colormap = matplotlib.colors.ListedColormap(colors)\n",
    "plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\n",
    "plt.title(\"Amazon ratings visualized in language using t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23c6ff-a38f-4990-a517-7e271cd4d8ef",
   "metadata": {},
   "source": [
    "### Embedding as a text feature encoder for ML algorithms\n",
    "An embedding can be used as a general free-text feature encoder within a machine learning model. Incorporating embeddings will improve the performance of any machine learning model, if some of the relevant inputs are free text. An embedding can also be used as a categorical feature encoder within a ML model. This adds most value if the names of categorical variables are meaningful and numerous, such as job titles. Similarity embeddings generally perform better than search embeddings for this task.\n",
    "\n",
    "We observed that generally the embedding representation is very rich and information dense. For example, reducing the dimensionality of the inputs using SVD or PCA, even by 10%, generally results in worse downstream performance on specific tasks.\n",
    "\n",
    "This code splits the data into a training set and a testing set, which will be used by the following two use cases, namely regression and classification.\n",
    "\n",
    "#### PCA (Principal Component Analysis) and SVD (Singular Value Decomposition) Explained\n",
    "Both PCA and SVD are techniques used for dimensionality reduction, which means reducing the number of features (dimensions) while preserving as much information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67311674-f59f-4250-8774-4fa8b45e1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df.ada_embedding.values),\n",
    "    df.Score,\n",
    "    test_size = 0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c2513-c646-48d4-bdd9-7b8aa31c4b35",
   "metadata": {},
   "source": [
    "### Regression using the embedding features\n",
    "Embeddings present an elegant way of predicting a numerical value. In this example we predict the reviewer’s star rating, based on the text of their review. Because the semantic information contained within embeddings is high, the prediction is decent even with very few reviews.\n",
    "\n",
    "We assume the score is a continuous variable between 1 and 5, and allow the algorithm to predict any floating point value. The ML algorithm minimizes the distance of the predicted value to the true score, and achieves a mean absolute error of 0.39, which means that on average the prediction is off by less than half a star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43fc93b-9929-493f-b919-14aa10079254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import randomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimator=100) #n_estimators specifies the number of decision trees in the random forest.\n",
    "rfr.fit(X_train, y_train)\n",
    "preds = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb15d3-ac6f-4ab2-92b2-6dd865c5d430",
   "metadata": {},
   "source": [
    "### Classification using the embedding features\n",
    "This time, instead of having the algorithm predict a value anywhere between 1 and 5, we will attempt to classify the exact number of stars for a review into 5 buckets, ranging from 1 to 5 stars.\n",
    "After the training, the model learns to predict 1 and 5-star reviews much better than the more nuanced reviews (2-4 stars), likely due to more extreme sentiment expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfe20b-4a0b-494c-b6ca-aa92c989ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c4ed5-736f-4604-a9db-eed85b368ace",
   "metadata": {},
   "source": [
    "### Zero-shot classification\n",
    "We can use embeddings for zero shot classification without any labeled training data. For each class, we embed the class name or a short description of the class. To classify some new text in a zero-shot manner, we compare its embedding to all class embeddings and predict the class with the highest similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d35b4-df41-4e82-9b24-8facdf292123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import cosine_similarity, get_embedding\n",
    "\n",
    "df= df[df.Score!=3]\n",
    "df['sentiment'] = df.Score.replace({1:'negative', 2:'negative', 4:'positive', 5:'positive'})\n",
    "\n",
    "labels = ['negative', 'positive']\n",
    "label_embeddings = [get_embedding(label, model=model) for label in labels]\n",
    "\n",
    "def label_score(review_embedding, label_embeddings):\n",
    "    return cosine_similarity(review_embedding, label_embeddings[1]) - cosine_similarity(review_embedding, label_embeddings[0])\n",
    "\n",
    "prediction = 'positive' if label_score('Sample Review', label_embeddings) > 0 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6c693-16a2-43cf-94cb-ed25b4df68d6",
   "metadata": {},
   "source": [
    "### User and product embeddings\n",
    "We can obtain a user embedding by averaging over all of their reviews. Similarly, we can obtain a product embedding by averaging over all the reviews about that product. In order to showcase the usefulness of this approach we use a subset of 50k reviews to cover more reviews per user and per product.\n",
    "\n",
    "We evaluate the usefulness of these embeddings on a separate test set, where we plot similarity of the user and product embedding as a function of the rating. Interestingly, based on this approach, even before the user receives the product we can predict better than random whether they would like the product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe8716-92f4-4ea3-aa1d-52d1609c1bbf",
   "metadata": {},
   "source": [
    "![Alt Text](D:/Projects/DeepLearning/FineTuning/images/boxPlot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8270ad62-f794-43aa-ba8b-8fa5424012b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = df.groupby('UserId').ada_embedding.apply(np.mean)\n",
    "prod_embeddings = df.groupby('ProductId').ada_embedding.apply(np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953192f-33d8-4acf-81d9-d1c699dafd0e",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "Clustering is one way of making sense of a large volume of textual data. Embeddings are useful for this task, as they provide semantically meaningful vector representations of each text. Thus, in an unsupervised way, clustering will uncover hidden groupings in our dataset.\n",
    "\n",
    "In this example, we discover four distinct clusters: one focusing on dog food, one on negative reviews, and two on positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd176a-6ee4-4a3f-8306-b60b450125c0",
   "metadata": {},
   "source": [
    "![clusters](D:/Projects/DeepLearning/FineTuning/images/clusters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1bead-38cb-483e-9bc8-a526448befca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "matrix = np.vstack(df.ada_embedding.values)\n",
    "n_clusters = 4 # If unsure, use the Elbow Method or Silhouette Score to find the best k.\n",
    "\n",
    "kmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\n",
    "kmeans.fit(matrix)\n",
    "df['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088f582-1f39-4247-943d-6e32f801312e",
   "metadata": {},
   "source": [
    "### How can I tell how many tokens a string has before I embed it?\n",
    "In Python, you can split a string into tokens with OpenAI's tokenizer tiktoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63140538-8978-4b96-b07b-f7140a7fb181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\") #\"cl100k_base\" is an encoding used by OpenAI models like GPT-4 and GPT-3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d8abc-5145-4767-8040-8f4035116e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
